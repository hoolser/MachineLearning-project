{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Challenge - Πρόβλεψη Πληρότητας Πτήσεων"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Η παρούσα εργασία έγινε στα πλαίσια του μαθήματος \"Εξόρυξη Γνώσης από Βάσεις Δεδομένων και τον Παγκόσμιο Ιστό\", από τους φοιτητές Αναστάσιο Τσούκα ΑΜ: 3140213, Βασίλη Σταυριανουδάκη ΑΜ: 3140193 και Έκτορα Τυπάλδο ΑΜ: 3140216.</b>\n",
    "\n",
    "Αρχικά πριν ξεκινήσουμε να γράφουμε κώδικα για την διαμόρφωση του προγράμματος, η πρώτη μας προσέγγιση ήταν να μελετήσουμε τα δεδομένα. Προσπαθήσαμε να επεξεργαστούμε τα δεδομένα με όσο το δυνατόν καλύτερο και πιο αποδοτικό τρόπο. Όπως θα δούμε και στην συνέχεια, παρατηρήσαμε ότι μερικά από τα features μας ήταν επικαλυπτόμενα με άλλα, ενώ από άλλα δημιουργήσαμε καινούργια."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην αρχή βάζουμε τα απαραίτητα import που θα μας χρειαστούνε, φορτώνουμε το excel train.csv που περιέχει τα δεδομένα μας και στον πίνακα y_train κρατάμε το feature PAX για να χρησιμοποιηθεί κατα την εκπαίδευση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from sklearn.metrics import f1_score\n",
    "import datetime\n",
    "\n",
    "dataset = pd.read_csv('train.csv')\n",
    "y_train = dataset[['PAX']]\n",
    "y_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Συγκεκριμένα κρατώντας τις στήλες Departure,Arrival όπου περιέχουν τον κωδικό του κάθε αεροδρομίου είδαμε ότι δεν χρειαζόταν να κρατήσουμε τις CityDeparture και CityArrival καθώς ήταν επικαλυπτόμενα. Οι παραπάνω στήλες θα αφαιρεθούν στην συνέχεια του κώδικα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Από τις στήλες LongitudeArrival, LatitudeArrival, LongitudeDeparture, LatitudeDeparture δημιουργήσαμε μια καινούργια στήλη την distance, η οποία είναι η απόσταση μεταξύ των δύο αεροδρομίων που αφορά η κάθε πτήση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "R = 6373.0\n",
    "dataset['dlon'] = np.radians(dataset['LongitudeArrival']) - np.radians(dataset.LongitudeDeparture)\n",
    "dataset['dlat'] = np.radians(dataset['LatitudeArrival']) - np.radians(dataset.LatitudeDeparture)\n",
    "\n",
    "dataset['a'] = np.sin(dataset[\"dlat\"].astype(np.float64)/2)**2 + np.cos(np.radians(dataset.LatitudeDeparture).astype(np.float64)) * np.cos(np.radians(dataset['LatitudeArrival']).astype(np.float64)) * np.sin(dataset[\"dlon\"].astype(np.float64)/2)**2\n",
    "dataset['c'] = 2 *np.arctan2(np.sqrt(dataset['a']).astype(np.float64), np.sqrt(1 - dataset['a']).astype(np.float64)).astype(np.float64) #atan2 h arctan2\n",
    "dataset['distance'] = R*dataset['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επίσης από το DateOfDeparture δημιουργήσαμε τρείς καινούργιες στήλες, μια για την ημέρα, μία για τον μήνα και μια για το έτος, τα οποία όπως θα δούμε και στην συνέχεια μας βοήθησαν να παράξουμε περαιτέρω στήλες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitDate = dataset[\"DateOfDeparture\"].str.split(\"-\", n = 3, expand = True)\n",
    "dataset['Day'] = splitDate[2]\n",
    "dataset['Month'] = splitDate[1] \n",
    "dataset['Year'] = splitDate[0]\n",
    "dataset['Month'] = dataset['Month'].astype(str).astype(int)\n",
    "dataset['Day'] = dataset['Day'].astype(str).astype(int)\n",
    "dataset['Year'] = dataset['Year'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ακόμα από την καινούργια στήλη Day που δημιουργήθηκε προηγουμένως, φτιάχνουμε ένα καινούργιο feature το οποίο είναι 0 αν είμαστε στις αρχές του μήνα, 1 αν είμαστε στο μέσο του μήνα και 2 αν βρισκόμαστε στο τέλος. Επίσης από την ίδια στήλη δημιουργήσαμε ένα ακόμα feature το DOW(day of week) που παίρνει τιμές από 0-6 ανάλογα με το ποια μέρα της εβδομάδας είναι.\n",
    "Επιπλέον από το καινούργιο feature DOW προσθέσαμε μια νέα Boolean στήλη, την WEEKDAY η οποία είναι 1 αν είναι Σαββατοκύριακο ή 0 διαφορετικά. Τέλος από την στήλη Month δημιουργήσαμε την στήλη Season η οποία αναπαριστά τις 4 εποχές του χρόνου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempArr = np.zeros(dataset.shape[0])\n",
    "for i in range(dataset.shape[0]):\n",
    "    if(dataset.iloc[i]['Day'] > 10 and dataset.iloc[i]['Day'] <= 20  ):\n",
    "        tempArr[i] = 1\n",
    "    elif(dataset.iloc[i]['Day'] <= 31 and dataset.iloc[i]['Day'] > 20):\n",
    "        tempArr[i] = 2\n",
    "        \n",
    "dataset['dayOfmonth'] = tempArr\n",
    "\n",
    "Year = dataset['Year'].tolist()\n",
    "Month = dataset['Month'].tolist()\n",
    "Day = dataset['Day'].tolist() \n",
    "DOW = list(range(len(Year)))\n",
    "for i in range(len(Year)):\n",
    "    DOW[i] = datetime.datetime( Year[i], Month[i], Day[i] ).weekday()\n",
    "DOWS = pd.Series(DOW)\n",
    "dataset['DOW'] = DOWS.values\n",
    "\n",
    "tempweek = np.zeros(dataset.shape[0])\n",
    "\n",
    "for i in range(dataset.shape[0]):\n",
    "    if(dataset.iloc[i]['DOW']==5 or dataset.iloc[i]['DOW']==6 ):\n",
    "        tempweek[i] = 1\n",
    "dataset['WEEKDAY'] = tempweek\n",
    "\n",
    "\n",
    "tempArr=np.zeros(dataset.shape[0])\n",
    "for i in range(dataset.shape[0]):\n",
    "    if(dataset.iloc[i]['Month']>=11 and dataset.iloc[i]['Month']<=1):\n",
    "        tempArr[i]=0 #xeimwnas\n",
    "    elif(dataset.iloc[i]['Month']>=2 and dataset.iloc[i]['Month']<=4):\n",
    "        tempArr[i]=1 # anoiksh\n",
    "    elif (dataset.iloc[i]['Month']>=5 and dataset.iloc[i]['Month']<=7):\n",
    "        tempArr[i]=2 # kalokairi\n",
    "    elif (dataset.iloc[i]['Month']>=8 and dataset.iloc[i]['Month']<=10):\n",
    "        tempArr[i]=3 # fthiniporo\n",
    "dataset['Season']=tempArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε αυτό το βήμα δημιουργούμε ένα καινούργιο feature το wtd/std_wtd, το οποίο προέκυψε από το WeeksToDeparture διά το std_wtd τα οποία διαπιστώσαμε ότι έχουν υψηλή συσχέτιση όπως θα δούμε παρακάτω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['wtd/std_wtd'] = dataset['WeeksToDeparture'] / dataset['std_wtd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σκεφτήκαμε ότι ενδεχομένως κατά τις περιόδους των διακοπών ή αργιών μπορεί να υπάρχει διαφοροποίηση στις τιμές PAX. Για αυτό το λόγο δημιουργήσαμε μια καινούργια Boolean στήλη, την holiday, η οποία είναι 1 αν είναι αργία αλλιώς είναι 0. Τέλος, αποφασίσαμε να ενώσουμε τις δύο στήλες Departure και Arrival δημιουργώντας την dep-arr στήλη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['DateOfDeparture'] = splitDate[0] + '-' + splitDate[1] + '-' + splitDate[2]\n",
    "\n",
    "import holidays \n",
    "hol = holidays.CA() + holidays.US() + holidays.MX() + holidays.TAR()\n",
    "h = np.zeros(dataset.shape[0])\n",
    "for i in range(dataset.shape[0]):\n",
    "    if(hol.get(dataset.iloc[i]['DateOfDeparture']) is not None):\n",
    "        h[i] = 1\n",
    "\n",
    "dataset['holiday'] = h       \n",
    "dataset['dep-arr'] = dataset['Departure'].astype(str) + dataset['Arrival']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να δημιουργήσουμε νέα χαρακτηριστικά, να βρούμε επικαλυπτώμενα, καθώς και να βρούμε τυχών συσχετίσεις μεταξύ τους, μεταξύ άλλων χρησιμοποιήσαμε το παρακάτω διαγράμματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"22.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με τις παρακάτω γραμμές αφήνουμε στον πίνακα dataset μόνο τα χαρακτηριστικά που θα χρησιμοποιήσουμε για την εκπαίδευση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Departure', 'Arrival', 'distance', 'Day', 'Month', 'Year', 'dayOfmonth', 'DOW', 'WEEKDAY', 'Season', 'wtd/std_wtd', 'holiday', 'dep-arr']\n"
     ]
    }
   ],
   "source": [
    "dataset.drop(dataset.columns[[0,2,3,4,6,7,8,9,10,11]], axis=1, inplace=True)\n",
    "dataset.drop(dataset.columns[[2,3,4,5]], axis=1, inplace=True)\n",
    "print(list(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για τις κατηγορικές μεταβλητές χρησιμοποιούμε LabelEncoder για την μετατροπή των αλφαριθμητικών σε αριθμούς. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dataset['Departure'] = le.fit_transform(dataset['Departure'])\n",
    "dataset['Arrival'] = le.fit_transform(dataset['Arrival'])\n",
    "dataset['dep-arr'] = le.fit_transform(dataset['dep-arr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Από τις ποσοτικές στήλες που είχαμε, η μοναδική η οποία είχε μεγάλες διακυμάνσεις στις τιμές ήταν η στήλη distance, στην οποία κάναμε normalize για να μεταφέρουμε τις τιμές της στο διάστημα [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = dataset[['distance']]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dataset['distance'] = x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"><b>Αντίστοιχα ότι κάναμε για την προετοιμασία των δεδομένων που χρησιμοποιούμε για train, κάνουμε και για τα δεδομένα του test.</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "#########################################\n",
    "R = 6373.0\n",
    "df_test['dlon'] = np.radians(df_test['LongitudeArrival']) - np.radians(df_test.LongitudeDeparture)\n",
    "df_test['dlat'] = np.radians(df_test['LatitudeArrival']) - np.radians(df_test.LatitudeDeparture)\n",
    "\n",
    "df_test['a'] = np.sin(df_test[\"dlat\"].astype(np.float64)/2)**2 + np.cos(np.radians(df_test.LatitudeDeparture).astype(np.float64)) * np.cos(np.radians(df_test['LatitudeArrival']).astype(np.float64)) * np.sin(df_test[\"dlon\"].astype(np.float64)/2)**2\n",
    "df_test['c'] = 2 *np.arctan2(np.sqrt(df_test['a']).astype(np.float64), np.sqrt(1 - df_test['a']).astype(np.float64)).astype(np.float64) #atan2 h arctan2\n",
    "df_test['distance'] = R*df_test['c']\n",
    "###########################################\n",
    "splitDate2 = df_test[\"DateOfDeparture\"].str.split(\"-\", n = 3, expand = True)\n",
    "df_test['Day'] = splitDate2[2]\n",
    "df_test['Month'] = splitDate2[1] \n",
    "df_test['Year'] = splitDate2[0]\n",
    "df_test['Month'] = df_test['Month'].astype(str).astype(int)\n",
    "df_test['Day'] = df_test['Day'].astype(str).astype(int)\n",
    "df_test['Year'] = df_test['Year'].astype(str).astype(int)\n",
    "\n",
    "tempArr=np.zeros(df_test.shape[0])\n",
    "for i in range(df_test.shape[0]):\n",
    "    if(df_test.iloc[i]['Day']>10 and df_test.iloc[i]['Day']<=20  ):\n",
    "        tempArr[i]=1\n",
    "    elif(df_test.iloc[i]['Day']<=31 and df_test.iloc[i]['Day']>20):\n",
    "        tempArr[i]=2\n",
    "        \n",
    "df_test['dayOfmonth']=tempArr        \n",
    "\n",
    "Year = df_test['Year'].tolist()\n",
    "Month = df_test['Month'].tolist()\n",
    "Day = df_test['Day'].tolist() \n",
    "DOW = list(range(len(Year)))\n",
    "for i in range(len(Year)):\n",
    "    DOW[i] = datetime.datetime( Year[i], Month[i], Day[i] ).weekday()\n",
    "DOWS = pd.Series(DOW)\n",
    "df_test['DOW'] = DOWS.values\n",
    "###################################################################\n",
    "tempweek = np.zeros(df_test.shape[0])\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    if(df_test.iloc[i]['DOW']==5 or df_test.iloc[i]['DOW']==6 ):\n",
    "        tempweek[i] = 1\n",
    "df_test['WEEKDAY'] = tempweek\n",
    "\n",
    "tempArr=np.zeros(df_test.shape[0])\n",
    "for i in range(df_test.shape[0]):\n",
    "    if(df_test.iloc[i]['Month']>=11 and df_test.iloc[i]['Month']<=1):\n",
    "        tempArr[i]=0 #xeimwnas\n",
    "    elif(df_test.iloc[i]['Month']>=2 and df_test.iloc[i]['Month']<=4):\n",
    "        tempArr[i]=1 # anoiksh\n",
    "    elif (df_test.iloc[i]['Month']>=5 and df_test.iloc[i]['Month']<=7):\n",
    "        tempArr[i]=2 # kalokairi\n",
    "    elif (df_test.iloc[i]['Month']>=8 and df_test.iloc[i]['Month']<=10):\n",
    "        tempArr[i]=3 # fthiniporo\n",
    "df_test['Season']=tempArr\n",
    "###############################################################\n",
    "df_test['wtd/std_wtd'] = df_test['WeeksToDeparture'] / df_test['std_wtd']\n",
    "\n",
    "df_test['DateOfDeparture'] = splitDate2[0] + '-' + splitDate2[1] + '-' + splitDate2[2]\n",
    "\n",
    "import holidays \n",
    "hol = holidays.CA() + holidays.US() + holidays.MX() + holidays.TAR()\n",
    "h = np.zeros(df_test.shape[0])\n",
    "for i in range(df_test.shape[0]):\n",
    "    if(hol.get(df_test.iloc[i]['DateOfDeparture']) is not None):\n",
    "        h[i] = 1\n",
    "\n",
    "df_test['holiday'] = h \n",
    "\n",
    "df_test.drop(df_test.columns[[0,2,3,4,6,7,8,9,10]], axis=1, inplace=True)\n",
    "df_test.drop(df_test.columns[[2,3,4,5]], axis=1, inplace=True)   \n",
    "\n",
    "df_test['dep-arr'] = df_test['Departure'].astype(str) + df_test['Arrival']\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_test['Departure'] = le.fit_transform(df_test['Departure'])\n",
    "df_test['Arrival'] = le.fit_transform(df_test['Arrival'])\n",
    "df_test['dep-arr'] = le.fit_transform(df_test['dep-arr'])\n",
    "#####################################################normalize\n",
    "from sklearn import preprocessing\n",
    "x = df_test[['distance']]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df_test['distance'] = x_scaled\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε περίπτωση που θέλουμε να τρέξουμε τον κώδικα τοπικά για δοκιμές, αντί για το παραπάνω κομμάτι κώδικα, χρησιμοποιούμε αυτόν εδώ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cross_validation import train_test_split\n",
    "#dataset, df_test, y_train, y_test = train_test_split(dataset, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως βλέπουμε στην παρακάτω φωτογραφία το PAX 4 έχει ελάχιστα στοιχεία σε σχέση με τα υπόλοιπα, δηλαδή τα δεδομένα μας είναι imbalanced και αυτό έχει ως αποτέλεσμα σχεδόν να παραβλέπεται από τους αλγορίθμους εκπαίδευσης. Για τον λόγο αυτό, χρησιμοποιήσαμε την τεχνική SMOTE (Synthetic Minority Over-sampling Technique) για να παράξουμε νέα συνθετικά δεδομένα με PAX 4. Στην συνέχεια επειδή τα δεδομένα που παράγονται βγαίνουν κατά προσέγγιση από τα πραγματικά δεδομένα, αυτό έχει ως αποτέλεσμα ορισμένες τιμές να μην είναι ακέραιες και γι’ αυτό το λόγο τους κάνουμε στρογγυλοποίηση."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"33.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeTrain = dataset.shape[0]\n",
    "cols = list(dataset)        \n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(ratio='minority')\n",
    "dataset, y_train = smote.fit_sample(dataset, y_train)\n",
    "\n",
    "shapeSmote = dataset.shape[0]\n",
    "\n",
    "for i in range(round((shapeSmote - shapeTrain) * 5/6)):\n",
    "    dataset = dataset[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "dataset.columns = cols\n",
    "############################### Round up\n",
    "for i in range(0 , dataset.shape[0]):\n",
    "    for j in range(0 , dataset.shape[1]):\n",
    "        if(dataset.columns[j] != 'distance' and dataset.columns[j] != 'wtd/std_wtd'):            \n",
    "            dataset.iloc[i,j] = round(dataset.iloc[i,j])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στην συνέχεια περνάμε όλα μας τα δεδομένα από τον OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_train = dataset\n",
    "X_test = df_test\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(dataset)  \n",
    "X_train = enc.transform(dataset)\n",
    "X_test = enc.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σαν τελικό μοντέλο επιλέξαμε το παρακάτω νευρωνικό δίκτυο για την εκπαίδευση των δεδομένων το οποίο αποτελείται από:\n",
    "\n",
    "-Το πρώτο layer χρησιμοποιεί την activation function “relu” και δέχεται σαν είσοδο όλες τις στήλες του πίνακα X_train.\n",
    "\n",
    "-Το δεύτερο και το τρίτο layer χρησιμοποιούν πάλι την activation function “relu” και μετά από δοκιμές προσαρμόσαμε τους κόμβους τους σε 190 και 80 αντίστοιχα.\n",
    "\n",
    "-Για να αποφύγουμε το overfitting, προσθέσαμε για κάθε hidden layer Dropout 0.30.\n",
    "\n",
    "-Έπειτα, στο τελευταίο επίπεδο του δικτύου το οποίο έχει 8 εξόδους (μία για κάθε κλάση), χρησιμοποιήσαμε την  activation function “softmax” ώστε να μπορεί να γίνει πολλαπλή κατηγοριοποίηση. \n",
    "\n",
    "-Τέλος, κάναμε compile το μοντέλο μας με optimizer “rmsprop” ο οποίος μετά από δοκιμές μας έβγαζε τα καλύτερα αποτελέσματα.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ironik\\Anaconda3\\envs\\DeepLeanring\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#Initializing Neural Network\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=X_train.shape[1], units=(190)))\n",
    "###gia to overfitting\n",
    "from keras.layers import Dropout\n",
    "classifier.add(Dropout(0.30))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=(80)))\n",
    "###gia to overfitting\n",
    "classifier.add(Dropout(0.30))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=8))\n",
    "\n",
    "\n",
    "# Compiling Neural Network\n",
    "classifier.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επειδή το τελευταίο layer του νευρωνικού δικτύου αποτελείται από 8 εξόδους (μία για κάθε PAX) πρέπει να μετατρέψουμε τα δεδομένα του y_train που θα δοθούν για την εκπαίδευση σε one hot τιμές.\n",
    "\n",
    "Στην συνέχεια εκπαιδεύουμε το μοντέλο μας με 30 εποχές (έπειτα από δοκιμές) και τα αποτελέσματα τα μετατρέπουμε από one hot τιμές σε κανονικά PAX με τιμές από 0-7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9240/9240 [==============================] - 1s 95us/step - loss: 1.6939 - acc: 0.2898\n",
      "Epoch 2/30\n",
      "9240/9240 [==============================] - 0s 49us/step - loss: 1.3384 - acc: 0.4275\n",
      "Epoch 3/30\n",
      "9240/9240 [==============================] - 0s 49us/step - loss: 1.2252 - acc: 0.4846\n",
      "Epoch 4/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 1.1533 - acc: 0.5077\n",
      "Epoch 5/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 1.0979 - acc: 0.5320\n",
      "Epoch 6/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 1.0622 - acc: 0.5527\n",
      "Epoch 7/30\n",
      "9240/9240 [==============================] - 0s 51us/step - loss: 1.0307 - acc: 0.5673\n",
      "Epoch 8/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.9906 - acc: 0.5776\n",
      "Epoch 9/30\n",
      "9240/9240 [==============================] - 0s 50us/step - loss: 0.9695 - acc: 0.5937\n",
      "Epoch 10/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.9397 - acc: 0.6026\n",
      "Epoch 11/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.9157 - acc: 0.6179\n",
      "Epoch 12/30\n",
      "9240/9240 [==============================] - 0s 48us/step - loss: 0.8826 - acc: 0.6323\n",
      "Epoch 13/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.8631 - acc: 0.6390\n",
      "Epoch 14/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.8500 - acc: 0.6476\n",
      "Epoch 15/30\n",
      "9240/9240 [==============================] - 0s 45us/step - loss: 0.8348 - acc: 0.6536\n",
      "Epoch 16/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.8131 - acc: 0.6605\n",
      "Epoch 17/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.7919 - acc: 0.6776\n",
      "Epoch 18/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.7845 - acc: 0.6773\n",
      "Epoch 19/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.7732 - acc: 0.6810\n",
      "Epoch 20/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.7528 - acc: 0.6898\n",
      "Epoch 21/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.7405 - acc: 0.6989\n",
      "Epoch 22/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.7337 - acc: 0.7037\n",
      "Epoch 23/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.7096 - acc: 0.7160\n",
      "Epoch 24/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.7098 - acc: 0.7177\n",
      "Epoch 25/30\n",
      "9240/9240 [==============================] - 0s 50us/step - loss: 0.6931 - acc: 0.7233\n",
      "Epoch 26/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.6980 - acc: 0.7197\n",
      "Epoch 27/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.6833 - acc: 0.7261\n",
      "Epoch 28/30\n",
      "9240/9240 [==============================] - 0s 48us/step - loss: 0.6658 - acc: 0.7371\n",
      "Epoch 29/30\n",
      "9240/9240 [==============================] - 0s 46us/step - loss: 0.6510 - acc: 0.7378\n",
      "Epoch 30/30\n",
      "9240/9240 [==============================] - 0s 47us/step - loss: 0.6520 - acc: 0.7426\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "from keras.utils import np_utils\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# Fitting our model \n",
    "classifier.fit(X_train, dummy_y,  epochs = 30)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "yPRED = np.zeros(y_pred.shape[0])\n",
    "for i in range(y_pred.shape[0]):\n",
    "    yPRED[i]=np.argmax(y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τέλος, δημιουργούμε το αρχείο για το submission στο Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_pred.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['Id', 'Label'])\n",
    "    for i in range(yPRED.shape[0]):\n",
    "        writer.writerow([i, yPRED[i].astype(np.int)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τον παρακάτω κώδικα τον χρησιμοποιούμε για να βγάλουμε τοπικά το f1 score καθώς και για να φτιάξουμε τον confusion matrix που μας βοηθάει να δούμε τις πραγματικές τιμές του κάθε PAX σε σχέση με τις προβλέψεις."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f1_score(y_test, yPRED, average='micro'))\n",
    "#from sklearn.metrics import confusion_matrix \n",
    "#cm = confusion_matrix(y_test, yPRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με τον παραπάνω αλγόριθμο εκπαίδευσης έχουμε μια απόδοση f1 score ανάμεσα στο 60-64% και συνολικά χρειάζεται περίπου 80 δευτερόλεπτα για να ολοκληρωθεί."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Αλγόριθμοι που δοκιμάσαμε κατα την ανάπτυξη της εργασίας"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Βέβαια, πριν ασχοληθούμε με τα νευρωνικά δίκτυα δοκιμάσαμε και άλλους αλγόριθμους ταξινόμησης όπως:\n",
    "\n",
    "(Στους πρώτους τρείς αλγορίθμους που θα σας παρουσιάσουμε η επεξεργασία των δεδομένων μας ήταν σε αρχικό στάδιο ακόμα και αυτός ίσως ήταν ένας λόγος όπου τα σκορ τους δεν είχαν σημαντικές βελτιώσεις)\n",
    "\n",
    "- K-nearest neighbors: ο συγκεκριμένος αλγόριθμος ήταν ο πρώτος που χρησιμοποιήσαμε και με διάφορες βελτιώσεις καταφέραμε να φτάσουμε το σκορ 42%.\n",
    "\n",
    "- Naïve Bayes: στην συνέχεια δοκιμάσαμε τον συγκεκριμένο αλγόριθμο αλλά για τον υπολογισμό των πιθανοτήτων δεν χρησιμοποιήσαμε έτοιμη συνάρτηση αλλά φτιάξαμε την υλοποίηση μόνοι μας. Το παραπάνω όμως δεν κατάφερε να βελτιώσει σημαντικά το σκορ.\n",
    "\n",
    "- Ensemble learning (stacking): Είχαμε την ιδέα να συνδυάσουμε περισσότερους από έναν αλγορίθμους έτσι ώστε να πετύχουμε κάποια βελτίωση στο σκορ. Στην συγκεκριμένη μέθοδο χωρίσαμε το dataset σε πέντε κομμάτια και εκπαιδεύσαμε πέντε διαφορετικά μοντέλα με κάθε ένα από αυτά. Στην συνέχεια, συνδυάσαμε τις προβλέψεις των πρώτων μοντέλων δίνοντάς τες στο επόμενο επίπεδο του αλγορίθμου που χρησιμοποιούσε το μοντέλο xgboost με σκοπό να βγάλει μια καλύτερη πρόβλεψη μαθαίνοντας από τις προηγούμενες. Βέβαια, δεν πετύχαμε ούτε εδώ κάποια σημαντική βελτίωση.\n",
    "\n",
    "(Σε αυτό το σημείο η επεξεργασία των δεδομένων μας είχε προχωρήσει πολύ και έτσι εκτός από την επιλογή του κατάλληλου μοντέλου κάναμε περισσότερες αλλαγές και επεξεργασία στα δεδομένα)\n",
    "\n",
    "- Logistic Regression: Έπειτα από την επεξεργασία των δεδομένων μας, για να δούμε αν οι αλλαγές μας βοηθάνε στις προβλέψεις θέλαμε να χρησιμοποιήσουμε έναν σχετικά απλό αλγόριθμο και γι’ αυτό τον λόγο τον επιλέξαμε. Παρατηρήσαμε ότι η επεξεργασία των δεδομένων μας όντως βοηθούμε καθώς με αυτόν τον αλγόριθμο είχαμε σκορ 49-50% το οποίο ήταν μια σημαντική βελτίωση σε σχέση με τους προηγούμενους αλγορίθμους.\n",
    "\n",
    "- GradientBoostingClassifier: Παρατηρήσαμε από το stacking ότι ο συγκεκριμένος αλγόριθμος ξεχώριζε σε σχέση με τους άλλους και γι’ αυτό τον λόγο αποφασίσαμε να τον δοκιμάσουμε με τα νέα δεδομένα. Έτσι καταφέραμε να βελτιώσουμε άλλο λίγο το σκορ και να το φτάσουμε 52-53%\n",
    "\n",
    "- Νευρωνικά Δίκτυα: Σε αυτό το σημείο αφού είμασταν σίγουροι για την επεξεργασία των δεδομένων μας προσπαθήσαμε να διαλέξουμε έναν καλύτερο αλγόριθμο, επιλέγοντας τα νευρωνικά δίκτυα. Ο συγκεκριμένος αλγόριθμος εκπαίδευσης μετά από πολλές δοκιμές για τις παραμέτρους του έφτασε να έχει το καλύτερο σκορ από όλους και γι’ αυτό τον λόγο τον κρατήσαμε.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
